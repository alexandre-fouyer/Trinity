import os
import math
import time
from typing import List

import streamlit as st
from openai import OpenAI

# Importe la logique m√©tier existante (le bot Trinity)
# Assure-toi que le fichier trinity_advanced_version.py est dans le m√™me dossier que cette app.
from trinity_advanced_version import TrinityBot

# ==========================
# üîß Utilitaires de chunking
# ==========================

def tokens_estimate(text: str) -> int:
    """Estimation simple (#tokens ‚âà #chars / 4). Suffisant pour d√©clencher le chunking.
    √âvite la d√©pendance √† tiktoken pour un d√©ploiement plus simple.
    """
    return max(1, math.ceil(len(text) / 4))


def chunk_text(text: str, max_tokens: int = 1500, overlap_tokens: int = 150) -> List[str]:
    """D√©coupe un texte en morceaux qui respectent un budget de tokens approximatif.
    On utilise une estimation 1 token ‚âà 4 caract√®res.
    """
    if not text:
        return []

    max_chars = max_tokens * 4
    overlap_chars = overlap_tokens * 4

    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + max_chars)
        chunk = text[start:end]
        chunks.append(chunk)
        if end == n:
            break
        start = end - overlap_chars  # chevauchement pour le contexte
        if start < 0:
            start = 0
    return chunks


# ======================================
# ü§ñ Appels OpenAI (unitaires et par lots)
# ======================================

def _openai_chat(client: OpenAI, model: str, system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
    """Effectue un appel Chat Completions simple (sans streaming) et renvoie le texte."""
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=temperature,
        max_tokens=800,
    )
    return resp.choices[0].message.content.strip()


def trinity_response_chunked(
    bot: TrinityBot,
    client: OpenAI,
    model: str,
    user_query: str,
    max_chunk_tokens: int = 1500,
    overlap_tokens: int = 150,
) -> str:
    """G√©n√®re la r√©ponse finale en envoyant le contexte par lots √† l'API OpenAI.

    √âtapes:
      1) Construction du prompt complet (r√®gles Trinity + contexte + question)
      2) Si le prompt est petit => un seul appel
      3) Sinon => d√©coupe du CONTEXTE en lots (chunking) et appels successifs
      4) Synth√®se finale sur la base de toutes les analyses partielles
    """
    # On reconstruit le prompt (m√™me logique que generate_response, mais ici on contr√¥le l'envoi par lots)
    context = bot.build_context(user_query)

    # Gestion "objection" et questions de qualification (on r√©utilise l'intelligence du bot)
    objection_keywords = ['cher', 'compliqu√©', 'difficile', 'pas s√ªr', 'h√©site']
    if any(k in user_query.lower() for k in objection_keywords):
        obj_reply = bot.handle_objection(user_query)
        context += f"\n\nR√âPONSE √Ä L'OBJECTION: {obj_reply}"

    if bot.session_context.conversation_stage == "discovery":
        q = bot.generate_qualification_questions()
        if q:
            context += f"\n\nQUESTIONS DE QUALIFICATION:\n{q}"

    system_prompt = f"""
Tu es Trinity, assistant expert Le Vapoteur Discount.

CONTEXTE SESSION:
- Profil: {bot.session_context.profile.value}
- Stage: {bot.session_context.conversation_stage}
- Budget: {bot.session_context.budget or 'non d√©fini'}
- Produits d√©j√† montr√©s: {len(bot.session_context.products_shown)}

R√àGLES:
1. Si le profil est DEBUTANT, √™tre tr√®s p√©dagogue
2. Si le profil est TRANSITION, insister sur les √©conomies
3. Si des questions de qualification sont pr√©sentes, les poser en priorit√©
4. Ne jamais reproposer les m√™mes produits
5. Toujours proposer des bundles/packs pour les d√©butants
6. Si objection d√©tect√©e, y r√©pondre avec empathie
7. Utiliser les URLs exactes fournies

COMPORTEMENT SELON LE STAGE:
- discovery: Poser des questions pour qualifier
- qualification: Affiner les besoins
- recommendation: Proposer 3-4 produits avec comparaison
- closing: Aider √† finaliser, ne pas reproposer

NE JAMAIS:
- R√©p√©ter les salutations
- Proposer plus de 4 produits
- Ignorer les objections
- Reproposer si le client veut commander
""".strip()

    base_user_prompt = (
        "Question: " + user_query + "\n\n" + context + "\n\n"
        "R√©ponds de mani√®re personnalis√©e selon le profil et le stage de conversation.\n"
        "Si c'est une objection, traite-la avec empathie.\n"
        "Si manque d'infos, pose les questions de qualification."
    )

    # Si c'est court, un seul call suffit
    if tokens_estimate(base_user_prompt) <= max_chunk_tokens:
        return _openai_chat(client, model, system_prompt, base_user_prompt)

    # Sinon, on d√©coupe UNIQUEMENT la partie contexte pour r√©duire la taille des lots
    # (on garde l'instruction et la question dans chaque lot)
    instruction_prefix = (
        "Tu vas recevoir des LOTS partiels du contexte.\n"
        "Pour chaque LOT, NE fournis PAS la r√©ponse finale.\n"
        "Renvoie plut√¥t un JSON compact avec ces cl√©s: \n"
        "- products: liste d'objets {id,name,price,url,is_priority,is_bundle}\n"
        "- facts: liste de puces utiles (max 8)\n"
        "- questions: questions de qualification manquantes (max 3)\n"
        "- objections: liste si pertinente\n"
        "N'inclus pas d'autre prose.\n"
    )

    # D√©tecte la fronti√®re "Question: ...\n\n" vs CONTEXTE et d√©coupe seulement le contexte lourd
    # Ici, on d√©coupe simplement 'context' (car base_user_prompt = Question + context + consignes)
    chunks = chunk_text(context, max_tokens=max_chunk_tokens, overlap_tokens=overlap_tokens)

    partial_jsons: List[str] = []
    progress = st.progress(0.0, text="Analyse par lots en cours‚Ä¶")

    for i, ch in enumerate(chunks, start=1):
        lot_prompt = (
            f"Question: {user_query}\n\n" + instruction_prefix + "\n=== LOT ===\n" + ch
        )
        with st.status(f"Traitement du lot {i}/{len(chunks)}", expanded=False):
            part = _openai_chat(client, model, system_prompt, lot_prompt)
            partial_jsons.append(part)
        progress.progress(i / len(chunks))
        time.sleep(0.1)

    # Synth√®se finale
    synth_instructions = (
        "Voici les analyses partielles (JSON) issues de plusieurs LOTS.\n"
        "Fusionne-les intelligemment et r√©dige maintenant la R√âPONSE FINALE pour l'utilisateur, en suivant STRICTEMENT ces r√®gles:\n"
        "‚Ä¢ Maximum 4 produits, jamais les m√™mes que d√©j√† montr√©s.\n"
        "‚Ä¢ Si profil d√©butant -> p√©dagogie + inclure au moins 1 pack √©conomique.\n"
        "‚Ä¢ Si profil transition -> insister sur les √©conomies.\n"
        "‚Ä¢ Si objection d√©tect√©e -> r√©pondre avec empathie.\n"
        "‚Ä¢ Utiliser EXACTEMENT les URLs fournies.\n"
        "‚Ä¢ Si info manquante, poser 2-3 questions de qualification.\n"
        "‚Ä¢ R√©ponse en fran√ßais.\n"
    )

    final_prompt = (
        f"Question: {user_query}\n\n" + synth_instructions + "\n\n"
        "=== ANALYSES PARTIELLES ===\n" + "\n---\n".join(partial_jsons)
    )

    return _openai_chat(client, model, system_prompt, final_prompt)


# ==================
# üéõÔ∏è Interface UI
# ==================

st.set_page_config(page_title="Trinity (Streamlit)", page_icon="ü§ñ", layout="wide")

st.title("ü§ñ Trinity ‚Äì Conseiller Vente Vape (Streamlit)")

st.markdown(
    """
Cette application Streamlit embarque **Trinity**, votre assistant IA sp√©cialis√© pour Le Vapoteur Discount.

**Objectif** :
- Discuter avec Trinity (onglet *Chat Trinity*)
- Analyser de gros contenus en **envoyant par lots** √† l'API OpenAI (onglet *Analyse (Chunking)*)

> ‚ö†Ô∏è Ne **hard-code** pas tes cl√©s. Utilise le champ ci-dessous ou `st.secrets`.
"""
)

# -------- Sidebar: Config --------
with st.sidebar:
    st.header("Configuration")
    openai_api_key = st.text_input("OpenAI API Key", type="password")
    model = st.selectbox(
        "Mod√®le OpenAI",
        [
            "gpt-4o-mini",  # rapide/√©co
            "gpt-4.1-mini",
            "gpt-4.1",
            "gpt-4-turbo",
            "gpt-4",
        ],
        index=0,
    )

    st.divider()
    st.subheader("PrestaShop ‚Äì Catalogue")
    default_url = "https://www.levapoteur-discount.fr"
    prestashop_url = st.text_input("URL PrestaShop", value=default_url)
    prestashop_key = st.text_input("Cl√© API PrestaShop", type="password")

    st.caption("Astuce: tu peux mettre tes secrets dans `.streamlit/secrets.toml`.")

# --------- Session State ----------
if "messages" not in st.session_state:
    st.session_state.messages = []  # liste de dicts {role, content}

if "trinity" not in st.session_state:
    st.session_state.trinity = None  # instance TrinityBot

if "catalog_loaded" not in st.session_state:
    st.session_state.catalog_loaded = False

if openai_api_key:
    client = OpenAI(api_key=openai_api_key)
else:
    client = None

# ===============
# üß† Tabs UI
# ===============

chat_tab, chunk_tab = st.tabs(["üí¨ Chat Trinity", "üì¶ Analyse (Chunking)"])

# --------------------
# üí¨ Chat Trinity
# --------------------
with chat_tab:
    st.subheader("Chat Trinity")

    colA, colB, colC = st.columns([1, 1, 2])
    with colA:
        init_click = st.button("Initialiser Trinity", type="primary")
    with colB:
        force_update = st.checkbox("Forcer mise √† jour du catalogue", value=False)
    with colC:
        auto_chunk = st.checkbox("Chunking automatique si prompt volumineux", value=True)

    max_chunk_tokens = st.slider("Budget tokens par lot", 800, 4000, 1500, step=100)
    overlap_tokens = st.slider("Chevauchement (tokens)", 50, 400, 150, step=10)

    if init_click:
        if not openai_api_key:
            st.error("Renseigne ta cl√© OpenAI dans la barre lat√©rale.")
        elif not prestashop_url or not prestashop_key:
            st.error("Renseigne l'URL et la cl√© API PrestaShop.")
        else:
            # Cr√©e/Met √† jour l'instance Trinity
            st.session_state.trinity = TrinityBot(
                prestashop_url=prestashop_url,
                prestashop_key=prestashop_key,
                openai_api_key=openai_api_key,
            )
            st.success("Trinity initialis√© ‚úÖ")

    # Charger le catalogue si besoin
    if st.session_state.trinity and (force_update or not st.session_state.catalog_loaded):
        with st.spinner("Chargement du catalogue (cat√©gories, marques, produits)‚Ä¶"):
            st.session_state.trinity.load_catalog_data(force_update=force_update)
            st.session_state.catalog_loaded = True
        st.toast("Catalogue pr√™t ‚úÖ", icon="‚úÖ")

    # Affiche l'historique
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Zone d'entr√©e
    user_msg = st.chat_input("Pose ta question (ex: Je veux un kit pas cher, 12mg, fruit√©)‚Ä¶")

    if user_msg:
        if not client or not openai_api_key:
            st.error("Ajoute ton OpenAI API Key dans la barre lat√©rale.")
        elif not st.session_state.trinity:
            st.error("Clique sur *Initialiser Trinity* d'abord.")
        else:
            # Affiche la question
            st.session_state.messages.append({"role": "user", "content": user_msg})
            with st.chat_message("user"):
                st.markdown(user_msg)

            # G√©n√©ration de la r√©ponse
            with st.chat_message("assistant"):
                placeholder = st.empty()
                try:
                    if auto_chunk:
                        answer = trinity_response_chunked(
                            bot=st.session_state.trinity,
                            client=client,
                            model=model,
                            user_query=user_msg,
                            max_chunk_tokens=max_chunk_tokens,
                            overlap_tokens=overlap_tokens,
                        )
                    else:
                        # Fallback: m√©thode d'origine (un seul appel)
                        answer = st.session_state.trinity.generate_response(user_msg)

                    placeholder.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    placeholder.error(f"Erreur pendant l'appel OpenAI: {e}")

# -----------------------------
# üì¶ Analyse (Chunking de texte)
# -----------------------------
with chunk_tab:
    st.subheader("Analyse d'un gros texte ‚Äì Envoi par lots √† l'API OpenAI")
    st.caption(
        "Colle un long texte OU charge un .txt, puis √©cris ta consigne. L'app d√©coupe le texte en lots, "
        "appelle l'IA pour chaque lot, puis fait une synth√®se finale."
    )

    uploaded = st.file_uploader("Importer un fichier .txt (optionnel)", type=["txt"])
    pasted = st.text_area("‚Ä¶ou colle ton texte ici", height=200)
    instruction = st.text_input("Consigne (ex: R√©sume et propose 3 recommandations produits adapt√©es)")

    c1, c2, c3 = st.columns(3)
    with c1:
        lot_tokens = st.number_input("Tokens/lot", min_value=800, max_value=4000, value=1500, step=100)
    with c2:
        lot_overlap = st.number_input("Chevauchement", min_value=50, max_value=400, value=150, step=10)
    with c3:
        synth_model = st.selectbox("Mod√®le (analyse)", [model, "gpt-4o-mini", "gpt-4.1-mini", "gpt-4.1", "gpt-4-turbo", "gpt-4"], index=0)

    run = st.button("Lancer l'analyse par lots", type="primary")

    if run:
        if not client or not openai_api_key:
            st.error("Ajoute ton OpenAI API Key dans la barre lat√©rale.")
        else:
            text = ""
            if uploaded is not None:
                try:
                    text = uploaded.read().decode("utf-8", errors="ignore")
                except Exception:
                    st.error("Impossible de lire le fichier. Assure-toi que c'est un .txt en UTF-8.")
            if not text:
                text = pasted

            if not text:
                st.warning("Aucun texte fourni.")
            elif not instruction:
                st.warning("Ajoute une consigne.")
            else:
                chunks = chunk_text(text, max_tokens=int(lot_tokens), overlap_tokens=int(lot_overlap))

                st.write(f"Texte total ‚âà {tokens_estimate(text)} tokens | {len(chunks)} lot(s)")
                partials = []

                system = (
                    "Tu es Trinity, assistant IA de Le Vapoteur Discount.\n"
                    "Analyse le lot pour r√©pondre √† la consigne finale. Ne donne pas la r√©ponse finale ici.\n"
                    "Retourne un JSON compact avec: facts[], products[], leads[], questions[]."
                )

                for i, ch in enumerate(chunks, start=1):
                    with st.status(f"Lot {i}/{len(chunks)} en cours‚Ä¶", expanded=False):
                        lot_prompt = (
                            f"Consigne Finale: {instruction}\n\n"
                            "Analyse UNIQUEMENT ce lot. Extrait l'essentiel utile pour la consigne.\n"
                            "Ne fais pas de redite.\n\n=== LOT ===\n" + ch
                        )
                        out = _openai_chat(client, synth_model, system, lot_prompt)
                        partials.append(out)

                st.success("Tous les lots sont analys√©s. Synth√®se en cours‚Ä¶")

                synth = (
                    f"Consigne Finale: {instruction}\n\n"
                    "Voici des sorties partielles (JSON). Fusionne-les et r√©dige une seule r√©ponse claire,\n"
                    "structur√©e, en fran√ßais, avec au plus 4 recommandations concr√®tes si pertinent.\n\n"
                    "=== SORTIES PARTIELLES ===\n" + "\n---\n".join(partials)
                )

                final = _openai_chat(client, synth_model, "Tu es Trinity, assistant IA.", synth)
                st.markdown("### R√©sultat final")
                st.markdown(final)

                with st.expander("Voir les sorties partielles (debug)"):
                    for idx, p in enumerate(partials, start=1):
                        st.markdown(f"**Lot {idx}**\n\n````\n{p}\n````")

st.markdown("---")
st.caption(
    "Astuce: lance l'app avec `streamlit run streamlit_trinity_app.py`.\n"
    "Le chunking est activ√© par d√©faut dans l'onglet *Chat Trinity* pour √©viter d'envoyer un √©norme bloc."
)
